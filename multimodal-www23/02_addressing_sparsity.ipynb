{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xLXWx4uhqtYJ"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PreferredAI/tutorials/blob/master/multimodal-www23/02_addressing_sparsity.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/PreferredAI/tutorials/blob/master/multimodal-www23/02_addressing_sparsity.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqktDa7H2hKz"
      },
      "source": [
        "# 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41oWCMUG2eC_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install --quiet cornac==1.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqKrDcGH2k7E",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "import cornac\n",
        "from cornac.utils import cache\n",
        "from cornac.datasets import filmtrust, amazon_clothing\n",
        "from cornac.eval_methods import RatioSplit\n",
        "from cornac.models import PMF, SoRec, WMF, CTR, BPR, VBPR\n",
        "from cornac.data import GraphModality, TextModality, ImageModality\n",
        "from cornac.data.text import BaseTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"System version: {sys.version}\")\n",
        "print(f\"Cornac version: {cornac.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "\n",
        "SEED = 42\n",
        "VERBOSE = True\n",
        "USE_GPU = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzPrmZOMtXt1"
      },
      "source": [
        "# 1. Multimodality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh3gYub8f6qZ"
      },
      "source": [
        "While preference data in the form of user-item interactions are the backbone of many recommender systems, such data tends to be sparse in nature. One way to address this sparsity is to look beyond the interaction data to the additional information associated with users or with items. The intuition is that items with similarity in \"content profiles\" would be correlated with similarity in preferences. Multimodality deals with how to model both preference data (one modality) and some content data either on user or item side (other modalities). In this tutorial, we see three forms of additional modalities, namely text, image, and graph, and investigate whether they add value to the the resulting recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8iXcfuKsVa8"
      },
      "source": [
        "## 1.1. Text Modality\n",
        "\n",
        "Often times, we are interested in building a recommender system for textual items (e.g., news, scientific papers), or items associated with text (e.g., titles, descriptions, reviews).  Text is informative and descriptive, therefore, exploiting textual information for better recommendations is an important topic in recommender systems.  In this tutorial, we introduce CTR [3], a recommendation model that combines matrix factorization and probablistic topic modeling. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "742yJ53Fj1Zz"
      },
      "source": [
        "### Collaborative Topic Regression (CTR)\n",
        "\n",
        "Under factorization framework, adoption prediction is in the form of $\\hat{r}_{i,j} = \\mathbf{u}_i^T \\mathbf{v}_j $.  The intuition in CTR model is that two items with similar topics would behave similarly. Thus, item latent factors $\\mathbf{v_j}$ is assumed to be drawn from a Normal distribution:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_j \\sim \\mathcal{N}(\\mathbf{\\theta}_j, \\lambda^{-1} \\mathbf{I})\n",
        "$$\n",
        "\n",
        "where the mean $\\mathbf{\\theta}_j$ is a vector indicating topic proportions of the item $j$. It is equivalent to:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{v}_j &= \\mathbf{\\theta}_j + \\mathbf{\\epsilon}_j \\\\\n",
        "\\mathbf{\\epsilon}_j &\\sim \\mathcal{N}(\\mathbf{0}, \\lambda^{-1} \\mathbf{I})\n",
        "\\end{align}\n",
        "\n",
        "Please refer to paper [3] for the generative process of CTR model.\n",
        "\n",
        "\n",
        "CTR also extends matrix factorization, in which the base model is WMF under implicit feedback setting. The adoption $p_{i,j}$ and confidence $c_{i,j}$ are defined as follows: \n",
        "\n",
        "\\begin{equation}\n",
        "p_{i,j} = \n",
        "\\begin{cases} \n",
        "r_{i, j} &\\mbox{if } r_{i,j} > 0 \\\\\n",
        "0 & \\mbox{otherwise} \n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "c_{i,j} = \n",
        "\\begin{cases} \n",
        "a & \\mbox{if } r_{i,j} > 0 \\\\\n",
        "b & \\mbox{otherwise }\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "The learning of CTR model is done via minimizing the following negative log-likelihood:\n",
        "\n",
        "$$ \\mathcal{L}(\\mathbf{U,V,\\theta, \\beta}|\\lambda) = \\frac{1}{2} \\sum_{i,j} c_{i,j} (p_{i,j} - \\mathbf{u}_i^T \\mathbf{v}_j)^2 - \\sum_{j}\\sum_{n} \\log \\big( \\sum_{k=1}^K \\mathbf{\\theta}_{j,k} \\mathbf{\\beta}_{k,w_{jn}} \\big) + \\frac{\\lambda}{2} \\sum_{i=1}^{N} ||\\mathbf{u}_i||^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{M} (\\mathbf{v}_j - \\mathbf{\\theta}_j)^T (\\mathbf{v}_j - \\mathbf{\\theta}_j) $$\n",
        "\n",
        "It is an iterative procedure of alternating between three steps:\n",
        "- Optimize for user and item latent vectors, $\\mathbf{u}_i$ and $\\mathbf{v}_j$, based on the current topic proportions $\\mathbf{\\theta}_j$.  \n",
        "- Optimize for topic proportions $\\mathbf{\\theta}_j$ based on the current vectors $\\mathbf{u}_i$ and $\\mathbf{v}_j$ and topic words $\\mathbf{\\beta}_k$.\n",
        "- Optimize for topic words $\\mathbf{\\beta}_k$ based on the current topic proportions $\\mathbf{\\theta}_i$.\n",
        "\n",
        "Let's experiment with two models CTR and WMF on a dataset from Amazon Clothing category.  Using this dataset, CTR will learn topics from item description.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ2Al6omrlEP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "K = 20\n",
        "ctr = CTR(k=K, max_iter=50, a=1.0, b=0.01, lambda_u=0.01, lambda_v=0.01, verbose=VERBOSE, seed=SEED)\n",
        "wmf = WMF(k=K, max_iter=50, a=1.0, b=0.01, learning_rate=0.005, lambda_u=0.01, lambda_v=0.01, \n",
        "          verbose=VERBOSE, seed=SEED)\n",
        "\n",
        "ratings = amazon_clothing.load_feedback()\n",
        "docs, item_ids = amazon_clothing.load_text()\n",
        "\n",
        "item_text_modality = TextModality(\n",
        "    corpus=docs,\n",
        "    ids=item_ids,\n",
        "    tokenizer=BaseTokenizer(sep=\" \", stop_words=\"english\"),\n",
        "    max_vocab=5000,\n",
        "    max_doc_freq=0.5,\n",
        ")\n",
        "\n",
        "ratio_split = RatioSplit(\n",
        "    data=ratings,\n",
        "    test_size=0.2,\n",
        "    rating_threshold=4.0,\n",
        "    exclude_unknowns=True,\n",
        "    item_text=item_text_modality,\n",
        "    verbose=VERBOSE,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "rec_50 = cornac.metrics.Recall(50)\n",
        "\n",
        "cornac.Experiment(eval_method=ratio_split, models=[ctr, wmf], metrics=[rec_50]).run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uofgWV3oFLRq"
      },
      "source": [
        "The results show that CTR model performs significantly better than WMF model in terms of Recall@50, which is due to the contribution of items' textual information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1AQfgGe8fyo"
      },
      "source": [
        "## 1.2. Image Modality\n",
        "\n",
        "In some contexts, item images are informative (e.g., fashion). With the existence of effective methods to learn image representation, using item images in recommender systems is gaining popularity. In this tutorial, we present VBPR [4], a recommendation model making use of item image features extracted from pre-trained Convolutional Neural Network (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7WGq2xxjqjA"
      },
      "source": [
        "### Visual Bayesian Personalized Ranking (VBPR)\n",
        "\n",
        "VBPR, which is also based on matrix factorization, is an extension of BPR model.  The novelty of VBPR is on how item visual features incorporated into the matrix factorization framework.  The preference score user $i$ giving to item $j$ is predicted as follows:\n",
        "\n",
        "$$\n",
        "\\hat{r}_{i,j} = \\alpha + b_i + b_j + \\mathbf{u}_i^T \\mathbf{v}_j + \\mathbf{p}_{i}^T(\\mathbf{E} \\times \\mathbf{f}_j) + \\mathbf{\\Theta}^T \\mathbf{f}_j\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\alpha, b_i, b_j$ are global bias, user bias, and item bias, respectively\n",
        "- $\\mathbf{u}_i \\in \\mathbb{R}^K$ and $\\mathbf{v}_j \\in \\mathbb{R}^K$ are user and item latent vectors, respectively\n",
        "- $\\mathbf{f}_j \\in \\mathbb{R}^D$ is the item image feature vector\n",
        "- $\\mathbf{p}_i \\in \\mathbb{R}^Q$ is user visual preference, and $(\\mathbf{E} \\times \\mathbf{f}_j) \\in \\mathbb{R}^Q$ is item visual representation with $\\mathbf{E} \\in \\mathbb{R}^{K \\times D}$ is the projection from visual feature space into preference space\n",
        "- $\\mathbf{\\Theta} \\in \\mathbb{R}^D$ is global visual bias vector\n",
        "\n",
        "Learning parameters of VBPR model can be done, similarly to BPR, via minimizing the following negative log-likelihood:\n",
        "\n",
        "$$ \\mathcal{L}(\\mathbf{U,V,b,E,\\Theta, P}|\\lambda) = \\sum_{(j >_i l) \\in \\mathbf{S}} \\ln (1 + \\exp\\{-(\\hat{r}_{i,j} - \\hat{r}_{i,l})\\}) + \\frac{\\lambda}{2} \\sum_{i=1}^{N} (||\\mathbf{u}_i||^2 + ||\\mathbf{p}_i||^2) + \\frac{\\lambda}{2} \\sum_{j=1}^{M} (b_j + ||\\mathbf{v}_j||^2) + \\frac{\\lambda}{2} ||\\mathbf{\\Theta}||^2 + \\frac{\\lambda}{2} ||\\mathbf{E}||^2_2 $$\n",
        "\n",
        "Noted that global bias $\\alpha$ and user bias $b_i$ do not affect the ranking of items, thus they are redundant and removed from the model parameters.\n",
        "\n",
        "Let's compare VBPR and BPR models with an experiment on Amazon Clothing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LQDDr1e8j6L",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "K = 10\n",
        "vbpr = VBPR(k=K, k2=K, n_epochs=50, batch_size=100, learning_rate=0.001,\n",
        "            lambda_w=1, lambda_b=0.01, lambda_e=0.0, use_gpu=True, verbose=VERBOSE, seed=SEED)\n",
        "bpr = BPR(k=(K * 2), max_iter=50, learning_rate=0.001, lambda_reg=0.001, verbose=VERBOSE, seed=SEED)\n",
        "\n",
        "ratings = amazon_clothing.load_feedback()\n",
        "img_features, item_ids = amazon_clothing.load_visual_feature()\n",
        "\n",
        "item_image_modality = ImageModality(features=img_features, ids=item_ids, normalized=True)\n",
        "\n",
        "ratio_split = RatioSplit(\n",
        "    data=ratings,\n",
        "    test_size=0.2,\n",
        "    rating_threshold=4.0,\n",
        "    exclude_unknowns=True,\n",
        "    item_image=item_image_modality,\n",
        "    verbose=VERBOSE,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "auc = cornac.metrics.AUC()\n",
        "\n",
        "cornac.Experiment(eval_method=ratio_split, models=[vbpr, bpr], metrics=[auc]).run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR6coclHv1Do"
      },
      "source": [
        "The results show that VBPR obtains higher performance than BPR in terms of AUC. That can be attributed to the usage of item visual features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r76coF3ZyWxb"
      },
      "source": [
        "## 1.3. Graph Modality\n",
        "\n",
        "In recommender systems, graph can be used to represent user social network or item contexts (e.g., co-views, co-purchases).  In this tutorial, we take the former as an example and discuss SoRec [2], a representative model for this class of algorithms. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GePqE99j5nr"
      },
      "source": [
        "### Social Recommendation (SoRec)\n",
        "\n",
        "SoRec model is based on matrix factorization framework. The idea is fusing user-item rating matrix with the user’s social network.  In summary, the *user-item rating matrix* ($R$) and the *user-user graph adjacency matrix* ($G$) are factorized with shared users' latent factors.  The user latent vectors in $\\mathbf{U}$ are shared to capture both user preferences as well as their social connections.  The rating prediction is obtained as $\\hat{r}_{i,j} = \\mathbf{u}_i^T \\mathbf{v}_j$, similarly to PMF model.\n",
        "\n",
        "To learn the model parameters, we minimize the following loss function:\n",
        "\n",
        "$$ \\mathcal{L}(\\mathbf{U,V,Z}|\\lambda,\\lambda_C) = \\frac{1}{2} \\sum_{r_{i,j} \\in \\mathcal{R}} (r_{i,j} - \\mathbf{u}_i^T \\mathbf{v}_j)^2 + \\frac{\\lambda_C}{2} \\sum_{g_{i,h} \\in \\mathcal{G}} (g_{i,h} - \\mathbf{u}_i^T \\mathbf{z}_h)^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{N} ||\\mathbf{u}_i||^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{M} ||\\mathbf{v}_j||^2 + \\frac{\\lambda}{2} \\sum_{h=1}^{N} ||\\mathbf{z}_h||^2 $$\n",
        "\n",
        "where $\\lambda_C$ is the relative importance of the social network factorization and $\\lambda$ is the regularization weight. \n",
        "\n",
        "Let's do a comparison between SoRec and its base model PMF on [FilmTrust dataset](http://konect.cc/networks/librec-filmtrust-trust/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2iyzuFFs1Fq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "K = 20\n",
        "sorec = SoRec(k=K, max_iter=50, learning_rate=0.001, lambda_reg=0.001, lambda_c=3.0, verbose=VERBOSE, seed=SEED)\n",
        "pmf = PMF(k=K, max_iter=50, learning_rate=0.001, lambda_reg=0.001, verbose=VERBOSE, seed=SEED)\n",
        "\n",
        "ratings = filmtrust.load_feedback()\n",
        "trust = filmtrust.load_trust()\n",
        "\n",
        "user_graph_modality = GraphModality(data=trust)\n",
        "\n",
        "ratio_split = RatioSplit(\n",
        "    data=ratings,\n",
        "    test_size=0.2,\n",
        "    rating_threshold=4.0,\n",
        "    exclude_unknowns=True,\n",
        "    user_graph=user_graph_modality,\n",
        "    verbose=VERBOSE,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "mae = cornac.metrics.MAE()\n",
        "\n",
        "cornac.Experiment(eval_method=ratio_split, models=[sorec, pmf], metrics=[mae]).run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oQ7J1_r2CGT"
      },
      "source": [
        "From the experiment, we see that SoRec achieves lower (better) MAE score as compared to PMF.  This improvement should be explained by useful information from user social network captured inside the model predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usIa7YnUBXYc"
      },
      "source": [
        "# 2. Cross-Modal Utilization\n",
        "\n",
        "Multimodal recommender systems are commonly catalogued based on the type of auxiliary data (modality) they leverage, such as preference data plus user-network (social), user/item texts\n",
        "(textual), or item images (visual) respectively. One consequence of this siloization along modality lines is the tendency for virtual walls to arise between modalities. For instance, a model ostensibly designed for images would experiment with only the image modality, and compare to other models also purportedly designed\n",
        "for images. In turn, a text-based model would be\n",
        "compared to another text-based model, similarly\n",
        "with item graph. However, most multimodal recommendation algorithms are innately machine learning models that fit the preference data, aided by the auxiliary data as features\n",
        "in some form. While the raw representations of modalities\n",
        "may differ, the eventual representations used in the learning\n",
        "process may have commonalities in form (textual product\n",
        "description may be represented as term vectors, related\n",
        "items as a vector of adjacent graph neighbors, etc.). Indeed,\n",
        "if we peel off the layer of pre-processing steps specific to\n",
        "a modality, we find that, for most models, the underlying\n",
        "representation can accommodate other modalities. It is this aspect that we explore in this notebook, i.e., using a model for a modality different from the\n",
        "one it was originally designed for [5]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JWasjYVCV_H"
      },
      "source": [
        "## 2.1. Using CDL, VBPR and MCF with the Text Modality\n",
        "\n",
        "We consider the Amazon Clothing dataset consisting of user-item ratings and item content information (e.g., text, visual features, relations). For the purpose of the following experiment, assume that we are interested in item textual descriptions only. To leverage this modalidy we consider three different models, namely CDL [6], VBPR [3], and MCF [7]. While the former was originally and experimented with text auxiliary data, VBPR and MCF have been investigated for integrating visual and graph information respectively. The following code illustrates how Cornac [4] enables to use VBPR and MCF with text auxiliary data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQVt-nCZCctl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load and split the Amazon clothing dataset\n",
        "ratings = amazon_clothing.load_feedback()\n",
        "docs, item_ids = amazon_clothing.load_text()\n",
        "\n",
        "ratio_split = RatioSplit(\n",
        "    data=ratings,\n",
        "    test_size=0.2,\n",
        "    rating_threshold=4.0,\n",
        "    exclude_unknowns=True,\n",
        "    seed=SEED,\n",
        "    verbose=VERBOSE,\n",
        ")\n",
        "\n",
        "# obtain global mapping of item ID to synchronize across modalities \n",
        "item_id_map = ratio_split.global_iid_map\n",
        "\n",
        "# build item text modality using the item text corpus\n",
        "VOCAB_SIZE = 5000\n",
        "text_modality = TextModality(corpus=docs, ids=item_ids, max_vocab=VOCAB_SIZE)\n",
        "text_modality.build(id_map=item_id_map)\n",
        "\n",
        "# here we use term-frequency matrix from item text as features, other choices available\n",
        "item_ids = list(item_id_map.keys())\n",
        "tf_mat = text_modality.count_matrix.A  # term-frequency matrix\n",
        "tf_mat = tf_mat[:len(item_ids)]  # remove unknown items during data splitting\n",
        "\n",
        "# build image modality with the term-frequency matrix as features \n",
        "image_modality = ImageModality(features=tf_mat, ids=item_ids)\n",
        "image_modality.build(id_map=item_id_map)\n",
        "\n",
        "# build graph modality with the term-frequency matrix as features.\n",
        "# Under the hood this will construct a k-nearest neighbor graph of items, encoding textual similarities among them.\n",
        "graph_modality = GraphModality.from_feature(features=tf_mat, ids=item_ids, k=5, similarity=\"cosine\")\n",
        "graph_modality.build(id_map=item_id_map)\n",
        "\n",
        "\n",
        "# provide all built modalities for access by models during the experiment\n",
        "ratio_split.add_modalities(item_text=text_modality, \n",
        "                           item_image=image_modality, \n",
        "                           item_graph=graph_modality)\n",
        "\n",
        "\n",
        "cdl = cornac.models.CDL(k=50, autoencoder_structure=[200], vocab_size=VOCAB_SIZE, \n",
        "                        act_fn='tanh', max_iter=50, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "vbpr = cornac.models.VBPR(k=10, k2=40, n_epochs=50, use_gpu=USE_GPU, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "mcf = cornac.models.MCF(k=50, max_iter=50, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "\n",
        "recall = cornac.metrics.Recall(k=50)\n",
        "ndcg = cornac.metrics.NDCG(k=50)\n",
        "\n",
        "\n",
        "text_exp = cornac.Experiment(eval_method=ratio_split,  \n",
        "                             models=[cdl, vbpr, mcf],\n",
        "                             metrics=[recall, ndcg])\n",
        "text_exp.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLCDpsQBCfgf"
      },
      "source": [
        "## Question\n",
        "\n",
        "Without looking at the next sections, can you write a Cornac code to use the above three models with the Image-modility and the Graph-modality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFb4DkTmCtA7"
      },
      "source": [
        "## 2.2. Using CDL, VBPR and MCF with the Image Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWJJfRDcC1K-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "ratings = amazon_clothing.load_feedback()\n",
        "features, item_ids = amazon_clothing.load_visual_feature()\n",
        "\n",
        "\n",
        "# construct item modalities using the image features\n",
        "image_modality = ImageModality(features=features, ids=item_ids) \n",
        "text_modality = TextModality(features=features, ids=item_ids)\n",
        "graph_modality = GraphModality.from_feature(features=features, ids=item_ids, k=5, similarity=\"cosine\")\n",
        "\n",
        "\n",
        "# provide all modalities into evaluation method to synchronize the building process\n",
        "# as we don't have to build them separately for this case (available features)\n",
        "ratio_split = RatioSplit(\n",
        "    data=ratings,\n",
        "    test_size=0.2,\n",
        "    rating_threshold=4.0,\n",
        "    exclude_unknowns=True,\n",
        "    seed=SEED,\n",
        "    verbose=VERBOSE,\n",
        "    item_text=text_modality,\n",
        "    item_image=image_modality,\n",
        "    item_graph=graph_modality,\n",
        ")\n",
        "\n",
        "\n",
        "cdl = cornac.models.CDL(k=50, autoencoder_structure=[200], vocab_size=text_modality.feature_dim,\n",
        "                        act_fn='tanh', max_iter=50, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "vbpr = cornac.models.VBPR(k=10, k2=40, n_epochs=50, use_gpu=False, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "mcf = cornac.models.MCF(k=50, max_iter=50, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "\n",
        "recall = cornac.metrics.Recall(k=50)\n",
        "ndcg = cornac.metrics.NDCG(k=50)\n",
        "\n",
        "\n",
        "image_exp = cornac.Experiment(eval_method=ratio_split, \n",
        "                              models=[cdl, vbpr, mcf],\n",
        "                              metrics=[recall, ndcg])\n",
        "image_exp.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfL7NXDNCzKp"
      },
      "source": [
        "## 2.3. Using CDL, VBPR and MCF with Graph Modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZMDzlR_C7dt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "ratings = amazon_clothing.load_feedback()\n",
        "contexts = amazon_clothing.load_graph()\n",
        "\n",
        "\n",
        "ratio_split = RatioSplit(\n",
        "    data=ratings,\n",
        "    test_size=0.2,\n",
        "    rating_threshold=4.0,\n",
        "    exclude_unknowns=True,\n",
        "    seed=SEED,\n",
        "    verbose=VERBOSE,\n",
        ")\n",
        "\n",
        "# obtain global mapping of item ID to synchronize across modalities \n",
        "item_id_map = ratio_split.global_iid_map  \n",
        "item_ids = list(item_id_map.keys())\n",
        "\n",
        "# build item graph modality using the item contexts\n",
        "graph_modality = GraphModality(data=contexts).build(id_map=item_id_map)\n",
        "adj_mat = graph_modality.matrix.A  # item graph adjacency matrix\n",
        "\n",
        "# build text and image modalities with the adjacency matrix as features \n",
        "text_modality = TextModality(features=adj_mat, ids=item_ids).build(id_map=item_id_map)\n",
        "image_modality = ImageModality(features=adj_mat, ids=item_ids).build(id_map=item_id_map)\n",
        "\n",
        "\n",
        "# provide all built modalities for access by models during the experiment\n",
        "ratio_split.add_modalities(item_text=text_modality,\n",
        "                           item_image=image_modality,\n",
        "                           item_graph=graph_modality)\n",
        "\n",
        "\n",
        "cdl = cornac.models.CDL(k=50, autoencoder_structure=[200], vocab_size=text_modality.feature_dim,\n",
        "                        act_fn='tanh', max_iter=50, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "vbpr = cornac.models.VBPR(k=10, k2=40, n_epochs=50, use_gpu=USE_GPU, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "mcf = cornac.models.MCF(k=50, max_iter=50, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "\n",
        "recall = cornac.metrics.Recall(k=50)\n",
        "ndcg = cornac.metrics.NDCG(k=50)\n",
        "\n",
        "\n",
        "graph_exp = cornac.Experiment(eval_method=ratio_split, \n",
        "                              models=[cdl, vbpr, mcf],\n",
        "                              metrics=[recall, ndcg])\n",
        "graph_exp.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcBSgqSC99-"
      },
      "source": [
        "## 2.4. Results: Recall and NDCG Bar Plots\n",
        "\n",
        "To make it convenient to analyze the results of the experiments from sections 2—4, the following code generates the Recall and NDCG bar plots across models and modalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL1YRo14DElP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "res_df = defaultdict(list)\n",
        "for text_res, image_res, graph_res in zip(text_exp.result, image_exp.result, graph_exp.result):\n",
        "  assert text_res.model_name == image_res.model_name == graph_res.model_name\n",
        "  res_df[\"Model\"].extend([text_res.model_name] * 3)\n",
        "  res_df[\"Modality\"].extend([\"Text\", \"Image\", \"Graph\"])\n",
        "  res_df[recall.name].extend([text_res.metric_avg_results[recall.name],\n",
        "                              image_res.metric_avg_results[recall.name],\n",
        "                              graph_res.metric_avg_results[recall.name]])\n",
        "  res_df[ndcg.name].extend([text_res.metric_avg_results[ndcg.name],\n",
        "                            image_res.metric_avg_results[ndcg.name],\n",
        "                            graph_res.metric_avg_results[ndcg.name]])\n",
        "res_df = pd.DataFrame(res_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAErDGcGDGie"
      },
      "source": [
        "### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ermrzzTDIHl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sns.barplot(x=\"Model\", y=recall.name, hue=\"Modality\", palette=\"Set1\", data=res_df);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wiWqtzkDKyt"
      },
      "source": [
        "### NDCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmsHHU6GDNUZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sns.barplot(x=\"Model\", y=ndcg.name, hue=\"Modality\", palette=\"Set1\", data=res_df);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALVQI-6qDPOd"
      },
      "source": [
        "## Question\n",
        "\n",
        "Based on the above results, what can you infer regarding cross-modality utilization? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FmiuTYiP_pB"
      },
      "source": [
        "## References\n",
        "\n",
        "1.   Ma, H., Yang, H., Lyu, M. R., & King, I. (2008, October). Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management (pp. 931-940).\n",
        "2.   Wang, C., & Blei, D. M. (2011, August). Collaborative topic modeling for recommending scientific articles. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 448-456).\n",
        "3.   He, R., & McAuley, J. (2016, February). VBPR: visual bayesian personalized ranking from implicit feedback. In Thirtieth AAAI Conference on Artificial Intelligence.\n",
        "4.   Salah, A., Truong, Q. T., & Lauw, H. W. (2020). Cornac: A Comparative Framework for Multimodal Recommender Systems. J. Mach. Learn. Res., 21, 95-1. https://cornac.preferred.ai\n",
        "5.   Truong, Q. T., Salah, A., Tran, T. B., Guo, J., & Lauw, H. W. (2021). Exploring Cross-Modality Utilization in Recommender Systems. IEEE Internet Computing.\n",
        "6.   Wang, H., Wang, N., & Yeung, D. Y. (2015, August). Collaborative deep learning for recommender systems. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1235-1244).\n",
        "7.   Park, C., Kim, D., Oh, J., & Yu, H. (2017, April). Do\" Also-Viewed\" Products Help User Rating Prediction?. In Proceedings of the 26th International Conference on World Wide Web (pp. 1113-1122).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
